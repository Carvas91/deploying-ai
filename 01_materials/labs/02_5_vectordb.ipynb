{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b477c8",
   "metadata": {},
   "source": [
    "# Vectors, Vectors, Vectors\n",
    "\n",
    "As with many things in life, it all boils down to linear algebra and a few non-linear functions.\n",
    "\n",
    "Vector representations enables similarity calculations and we can think of several applications that follow from it: question answering, evaluation procedures, fetching related texts, etc. Because of this usefulness, we want to find efficient ways of 1) obtaining vector representations, 2) operating on them, and 3) storing them for later use.\n",
    "\n",
    "In this notebook, we will discuss how to obtain embeddings from OpenAI API and use a vector database to store and operate on vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../../05_src/.secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18829604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb5340",
   "metadata": {},
   "source": [
    "Our sample phrases cover three topics: freedom, friendship, and food."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    # Freedom\n",
    "    \"Freedom consists not in doing what we like, but in having the right to do what we ought.\",\n",
    "    \"Those who deny freedom to others deserve it not for themselves.\",\n",
    "    \"Liberty, when it begins to take root, is a plant of rapid growth.\",\n",
    "    \"Freedom lies in being bold.\",\n",
    "    \"Is freedom anything else than the right to live as we wish?\",\n",
    "    \"I am no bird and no net ensnares me: I am a free human being with an independent will.\",\n",
    "    \"The secret to happiness is freedom... And the secret to freedom is courage.\"\n",
    "    \"Freedom is the oxygen of the soul.\", \n",
    "    \"Life without liberty is like a body without spirit.\"\n",
    "    # Friendship\n",
    "    \"There is nothing on this earth more to be prized than true friendship.\",\n",
    "    \"There are no strangers here; Only friends you havenâ€™t yet met.\",\n",
    "    \"Friendship is the only cement that will ever hold the world together.\",\n",
    "    \"A true friend is someone who is there for you when he'd rather be anywhere else.\",\n",
    "    \"Friendship is the golden thread that ties the heart of all the world.\", \n",
    "    \"Your friend is the man who knows all about you and still likes you.\",\n",
    "    \"A single rose can be my garden... a single friend, my world.\"\n",
    "    # Food\n",
    "    \"One cannot think well, love well, sleep well, if one has not dined well.\",\n",
    "    \"Let food be thy medicine and medicine be thy food.\",\n",
    "    \"People who love to eat are always the best people.\",\n",
    "    \"The only way to get rid of a temptation is to yield to it.\",\n",
    "    \"Food is our common ground, a universal experience.\",\n",
    "    \"Life is uncertain. Eat dessert first.\",\n",
    "    \"All you need is love. But a little chocolate now and then doesn't hurt.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b773c1",
   "metadata": {},
   "source": [
    "We have 14 phrases in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9658cb78",
   "metadata": {},
   "source": [
    "To obtain embeddings, we will use the `text-embedding-3-small` model. This model generates 1536-dimensional vectors for each input text. \n",
    "\n",
    "The documentation for the embeddings API can be found [here](https://platform.openai.com/docs/guides/embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b15a4",
   "metadata": {},
   "source": [
    "# A Simple Input\n",
    "\n",
    "We first start with a simple example using the first document/phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b422338",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "response = client.embeddings.create(\n",
    "    input = phrases[0], \n",
    "    model = \"text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e5290",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9515ac",
   "metadata": {},
   "source": [
    "# Loop through Simple Inputs\n",
    "\n",
    "We will now try the example found in the [API documentation](https://platform.openai.com/docs/guides/embeddings/embeddings#obtaining-the-embeddings), which simply loops through the documents, calling the API each time. The function below first performs a simple cleanup (removes line breaks), then requests the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045b3c3",
   "metadata": {},
   "source": [
    "Using Python's list comprehension syntax, we can run the function for each of our example phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a094372",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [get_embedding(doc) for doc in phrases]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e8610",
   "metadata": {},
   "source": [
    "The statement above is roughly equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e066cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for doc in phrases:\n",
    "    doc_emb = get_embedding(doc)\n",
    "    embeddings.append(doc_emb)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8efcd",
   "metadata": {},
   "source": [
    "# Sending Lists of Inputs to the API\n",
    "\n",
    "We can also send a collection of inputs to the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "response = client.embeddings.create(\n",
    "    input = phrases, \n",
    "    model = \"text-embedding-3-small\"\n",
    ")\n",
    "response.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029bf18b",
   "metadata": {},
   "source": [
    "# Vector DB\n",
    "\n",
    "We can use a specialized database to store our embeddings, relate them to documents, and efficiently perform computations like cosine similarity.\n",
    "\n",
    "![](img/02_chroma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc994ef1",
   "metadata": {},
   "source": [
    "The document database that we will use for our experiments is Chroma DB, a simple implementation of Vector DB that is commonly used for prototyping. \n",
    "\n",
    "A few useful references are: \n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/docs/overview/introduction).\n",
    "- [ChromaDB Cookbook](https://cookbook.chromadb.dev/running/running-chroma/#chroma-cli).\n",
    "\n",
    "Chroma can be run locally in memory, locally using file persistence, or using a Docker container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ddc9e1",
   "metadata": {},
   "source": [
    "## Running Chroma Locally in Memory\n",
    "\n",
    "The simplest implementation is to run Chroma DB in memory without persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cefa71",
   "metadata": {},
   "source": [
    "First, create a collection. A collection is a container that groups documents together. A collection would be equivalent to a table which groups togher records in a relational database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name = \"nice_phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc83a87f",
   "metadata": {},
   "source": [
    "Then, add documents to our collection. Each document will contain:\n",
    "\n",
    "1. An identifier.\n",
    "2. The phrase.\n",
    "3. The embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [item.embedding for item in response.data]\n",
    "ids = [f\"id{i}\" for i in range(len(phrases))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adabc3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(embeddings = embeddings, \n",
    "               documents = phrases, \n",
    "               ids = ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8ff17",
   "metadata": {},
   "source": [
    "Now, we can use Chroma DB's [`query`](https://docs.trychroma.com/docs/querying-collections/query-and-get) method to perform a query using similarity search. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c3b580",
   "metadata": {},
   "source": [
    "## Performing a Search Using Custom Embeddings\n",
    "\n",
    "We could use a function such as the one below to provide our own embeddings of the query text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfeae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb(query, top_n = 2):\n",
    "    query_embedding = get_embedding(query)\n",
    "    results = collection.query(query_embeddings = [query_embedding], n_results = top_n)\n",
    "    return [(id, score, text) for id, score, text in zip(results['ids'][0], results['distances'][0], results['documents'][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23afe9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is good food?\"\n",
    "\n",
    "query_chromadb(query, top_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3689b80",
   "metadata": {},
   "source": [
    "## Performing a Search Using Embedding Function\n",
    "\n",
    "Alternatively, we can define the embedding function at the moment in which we create the collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ea640",
   "metadata": {},
   "source": [
    "If needed, list and remove any collection as you require:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85dd1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(\"nice_phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ad5c5",
   "metadata": {},
   "source": [
    "We can now re-use the collection name using an OpenAI embedding function. Notice that we pass the `api_key` parameter explicitly, as the environment variable name that holds the API key for Chroma DB and for the OpenAI library are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddaadb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name = \"nice_phrases\",\n",
    "    embedding_function = OpenAIEmbeddingFunction(\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model_name=\"text-embedding-3-small\")\n",
    ")\n",
    "collection.add(embeddings = embeddings, \n",
    "               documents = phrases, \n",
    "               ids = ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8a606",
   "metadata": {},
   "source": [
    "With the embedding function, we can now perform the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56edaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.query(\n",
    "    query_texts = [\"What is a friend?\", \"What is good food?\"], \n",
    "    n_results = 2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
